{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hppranaav02/bad-poets-society/blob/main/bad_poets_society.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvU3S2VwNfB7"
      },
      "source": [
        "# Generate poems though a creative statistical or stochaistic process\n",
        "<hr>\n",
        "\n",
        "## Inspiration\n",
        "After going through a few styles of poems we have settled on sticking to poems from the Nobel Laureate William Butler Yeats for their simple rhyme scheme and broad topics about life, and sonnets from William shakespeare for complexity in wording, structure and topics like love and passion (He was probably the Elvis Presley of his time).\n",
        "\n",
        "We came up with these choices partly because one of the group members studied Shakespeares' work [As you like it](https://www.shakespeare.org.uk/explore-shakespeare/shakespedia/shakespeares-plays/as-you-like-it/) in high school (_Side Note: It was fun but excrutiatingly intense_ ). During our conversations, he came up with the absurd idea of bringing Shakespeare to modern times and making him learn modern english. All to see what kind of literature he would produce with the present language.\n",
        "\n",
        "We want our system to generate a shakespearean style verse with a modern twist to it. We are still not sure how we will achieve this but we will take a simple approach first (maybe explore RiTa a bit) to get a feel.\n",
        "\n",
        "Some of the poems which we were motivated by are as follows\n",
        "\n",
        "\n",
        "\n",
        "**W. B. Yeats**\n",
        "\n",
        "* [Sailing to Byzantium](https://colab.research.google.com/drive/1eanny4nzn0HxlPce8Y1mpJ9__HsHdjRx#scrollTo=vvU3S2VwNfB7&line=7&uniqifier=1)\n",
        "\n",
        "* [The Stolen Child](https://www.poemhunter.com/poem/the-stolen-child/)\n",
        "\n",
        "* [A Prayer for my daughter](https://www.poemhunter.com/poem/a-prayer-for-my-daughter/)\n",
        "\n",
        "**W. Shakespeare**\n",
        "\n",
        "* [Sonnet 130 - \"My mistress' eye are nothing like the sun\"](https://www.poemhunter.com/poem/sonnet-130-my-mistress-eyes-are-nothing-like-the/)\n",
        "\n",
        "* [Sonnet 18 - \"Shall I compare thee to a summers day\"](https://www.poemhunter.com/poem/sonnet-18-shall-i-compare-thee-to-a-summer-s-day/)\n",
        "\n",
        "* [Sonnet 104 - To me, Fair friend, You can never be old](https://www.poemhunter.com/poem/sonnet-104-to-me-fair-friend-you-never-can-be-old/)\n",
        "\n",
        "## Creating the Poem Generator\n",
        "\n",
        " Right now we are evaluating how generator systems work. Markov Chains, we thought would be the right first step to try and generate poems given its simplicity.\n",
        "\n",
        "  * Markov Chains: Using RiTa with Java, we were able to get an idea of how markov chains produce output based on the inputs given to it. In our example we used poems from W.B. Yeats and sonnets from William Shakespeare. Our hopes were that we would be able to generate poems which  blend elizabethian and modern english to create an illusion of what shakespeare would write if he time travelled to the modern world and learnt modern english.\n",
        "\n",
        "  * We varied the temprature settings to pick up infrequent words with higher probability to see the results but that resulted in unusual outputs with no coherence.\n",
        "\n",
        "  * Based on our results, the output did not generate the text exactly what we were looking for. But the form of speech was good without structure. An example of the text generated by our [Markov chains](https://editor.p5js.org/hppranaav02/sketches/zDqu4DuOu) are:\n",
        "\n",
        "\n",
        "    (2-gram)\n",
        "    Mere dreams - and my verse alone did decree That then,\n",
        "    While up, bodies broken rings Upon the light, for thy dear perpetual dulness.\n",
        "    And what good; Wherefore I dropped the moon may deserve to the heights of my soul\n",
        "    Does Minnaloushe runs to work of his beams assemble?\n",
        "    The first of thralled discontent, I, The pacing to that men as sweet-season'd showers\n",
        "    There lives th 'eyes, and poets can understand.\n",
        "\n",
        "    (3-gram)\n",
        "    The mortal moon hath her wish, thou mine, mine eyes best see, Despite of wrinkles, this thy golden time.\n",
        "    What merit do I not spend Revenge upon myself, but with divining eyes, seeing this, let him bring forth Eternal numbers to outlive long date.\n",
        "    Then can I grieve at grievances foregone, And stars began to peep.\n",
        "    This other man I had dreamed A drunken, vainglorious lout.\n",
        "    Excuse not silence so, To swear against the cold, Bare ruined choirs, where thou art?\n",
        "\n",
        "We have decided to use a neural network, specifically a Recurrent Neural Network (RNN). A recurrent neural network fits well for text generation as its structure defines it to uses information persistence to allow information to be passed from one step to another. Normal neural networks have this exact shortcoming.\n",
        "\n",
        "* In RNNs, a special neural network which uses long short term memory, called LSTM, which we have decided to explore. As a RNN, LSTM have the same structure of self looping nodes with an addition of a cell state which helps pass information through its iterations. We finalised our stopping criteria to be the number of words being output from the system.\n",
        "\n",
        "* Tensorflow - This is required to import the LSTM neural network model and other tools to help format data\n",
        "* Pandas - Used to pre-process data which is then supplied to the neural network\n",
        "\n",
        "* For our data, it was initially just a set of poems from W.B. Yeats totalling 3000 words. The results of this were some quirky responses but the model broke down at about 30 words after which words would repeat with a high frequency.\n",
        "\n",
        "* Update: By increasing the epochs from 50 to 100 resulted in better text generation. By better we mean it is giving more coherent and meaningful results as compared to the Markov chains implementation.\n",
        "\n",
        "* While playing around, we figured out that we can influence the model a bit more such that we get a rhyme scheme in the output. This can be done via a custom loss function such that we can promote outputs with a rhyme based on a list of word pairs which rhyme. To build a custom loss function to promote rhyme schemes in the generated outputs, we took the help of ChatGPT as both of us do not have domain knowledge.\n",
        "\n",
        "* We have stopped looking into the custom loss function as it caused the the accuracy of the model training to fall to around 15%. The outputs also were in no way rhyming, which is the whole motive for this excercise.\n",
        "\n",
        "* Update: On discussion with Rob, we have increased the dataset to have around 11 thousand words. Due to this we are able to generate around 50 words (including the seed word(s)) before incoherence sets in. Through this we realised that for the data was more important as compared to the epochs we run this for. We reduced the epochs down to 75 and still got good results.\n",
        "\n",
        "* Our dataset now contains close to 30 thousand words of poetry from W. B. Yeats and William Shakespeare. The results are looking way better and quite poetic something like we were expecting of our efforts.\n",
        "\n",
        "We have decided that the LSTM implementation of the poetry generator will be our final system through which we generate, and further present the poems.\n",
        "\n",
        "## Measuring Creativity\n",
        "\n",
        "Based on the 2 methods that we have tried, we have kept the markov chain model the baseline with it being the floor of creative output. We made this choice as we viewed creativity as the novelty arising from knowledge learnt where markov chains was not completely novel as its output was determined by not only the data, but out choice of how the words are linked together through n-grams.\n",
        "\n",
        "LSTM provides a higher level of creativity as its output is determined by the patterns of the words we feed it. This gives a sense of novelty. The output was close to what we were looking for as the end result, and it was able to produce simillar results across multiple runs of the system. However the poems generated consist of elements of the poems that we provided it and cannot be considered a 100% original. Based on these facts we concur that our system has some attributes of creativity as per Ritchies' criteria but does not conform to it in its entirity.\n",
        "\n",
        "[One way](https://academic.oup.com/mind/article/LIX/236/433/986238) of looking at measuring creativity is to check whether a human judge or judges can tell the difference between a system generated poem and one by a human. Applying this logic to our system we would have to allow a human to look at a few poems one at a time where the poems shown are randomly mized between real world authors and system generated. Then ask them to categorize them as either. Average the values over all scores and then express the score of the generated poems as\n",
        "\n",
        "score = ( system generated poems categorized as real world poems) / (Total number of poems ranked)\n",
        "\n",
        "Based on SPECS outlined by [Jordanous](https://psycnet.apa.org/record/2012-25445-006), the criterias which we think should be used to evaluate our system should be Originality, Value and Domain Knowledge. Expansion of how the criterias would be used are as follows\n",
        "\n",
        "* **Originality**: In a way ,the poems generated from the system have adhered to the styles we provided it. This results in the a style which not only sounds poetic but can be interpreted with a deeper meaning. We wouldn't go as far as to say that there is something to be learnt from the outputs or that its results are profound but the end products are suitable and fitting to the domain we have chosen.\n",
        "\n",
        "* **Originality**: We feel that our model does not have originality completely as it does not come up with new ideas. However it does show different results every time it is run which gives it some backing on this point. Where it lacks is that it can only express the information we give it in multiple different combinations, and cannot provide new insights of it.\n",
        "\n",
        "* **Domain Knowledge**: In this space, our model excels as it only gets better with more information that we feed it. It does not learn or discover new knowledge by itself, but on applying data to it, we can enhance its technical abilities further.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This is the code in which we create and train the model"
      ],
      "metadata": {
        "id": "vbkE_Dwg0gQR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfOq6IkS1NGB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras.utils as ku\n",
        "from wordcloud import WordCloud\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "data = open('sample_data/poems.txt',encoding=\"utf-8\").read();\n",
        "\n",
        "# Generating the corpus by\n",
        "# splitting the text into lines\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "# get tokens out from lines\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "# Vocabulary count of the corpus\n",
        "total_words = len(tokenizer.word_index)\n",
        "\n",
        "# Converting the text into embeddings\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences,\n",
        "                                         maxlen=max_sequence_len,\n",
        "                                         padding='pre'))\n",
        "predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "label = ku.to_categorical(label, num_classes=total_words+1)\n",
        "\n",
        "# Building a Bi-Directional LSTM Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words+1, 100,\n",
        "                    input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words+1/2, activation='relu',\n",
        "                kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(total_words+1, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "#train model\n",
        "history = model.fit(predictors, label, epochs=150, verbose=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This is the code used to generate the poem"
      ],
      "metadata": {
        "id": "jKE3McIv3gF8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gdp-s5ZrcDB"
      },
      "outputs": [],
      "source": [
        "seed_text = \"Die\"\n",
        "next_words = 50 - len(seed_text)\n",
        "ouptut_text = \"\"\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences(\n",
        "        [token_list], maxlen=max_sequence_len-1,\n",
        "      padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list,\n",
        "                                        verbose=0), axis=-1)\n",
        "    output_word = \"\"\n",
        "\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
