{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hppranaav02/bad-poets-society/blob/main/bad_poets_society.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvU3S2VwNfB7"
      },
      "source": [
        "# Generate poems though a creative statistical or stochaistic process\n",
        "\n",
        "After going through a few styles of poems we have settled on sticking to poems from the Nobel Laureate William Butler Yeats for their simple rhyme scheme and broad topics about life, and sonnets from William shakespeare for complexity in wording, structure and topics like love and passion (He was probably the Elvis Presley of his time).\n",
        "\n",
        "We came up with these choices partly because one of the group members studied Shakespeares' work [As you like it](https://www.shakespeare.org.uk/explore-shakespeare/shakespedia/shakespeares-plays/as-you-like-it/) in high school (_Side Note: It was fun but excrutiatingly intense_). During our conversations, he came up with the absurd idea of bringing Shakespeare to modern times and making him learn modern english. All to see what kind of literature he would produce with the present language.\n",
        "\n",
        "We want our system to generate a shakespearean style verse with a modern twist to it. We are still not sure how we will achieve this but we will take a simple approach first (maybe explore RiTa a bit) to get a feel.  \n",
        "\n",
        "Some of the poems which we were motivated by are as follows\n",
        "\n",
        "W. B. Yeats\n",
        "\n",
        "* [Sailing to Byzantium](https://colab.research.google.com/drive/1eanny4nzn0HxlPce8Y1mpJ9__HsHdjRx#scrollTo=vvU3S2VwNfB7&line=7&uniqifier=1)\n",
        "\n",
        "* [The Stolen Child](https://www.poemhunter.com/poem/the-stolen-child/)\n",
        "\n",
        "* [A Prayer for my daughter](https://www.poemhunter.com/poem/a-prayer-for-my-daughter/)\n",
        "\n",
        "W. Shakespeare\n",
        "\n",
        "* [Sonnet 130 - \"My mistress' eye are nothing like the sun\"](https://www.poemhunter.com/poem/sonnet-130-my-mistress-eyes-are-nothing-like-the/)\n",
        "\n",
        "* [Sonnet 18 - \"Shall I compare thee to a summers day\"](https://www.poemhunter.com/poem/sonnet-18-shall-i-compare-thee-to-a-summer-s-day/)\n",
        "\n",
        "* [Sonnet 104 - To me, Fair friend, You can never be old](https://www.poemhunter.com/poem/sonnet-104-to-me-fair-friend-you-never-can-be-old/)\n",
        "\n",
        "\n",
        " Right now we are evaluating how generator systems work. Markov Chains, we thought would be the right first step to try and generate poems given its simplicity.\n",
        "\n",
        "  * Markov Chains: Using RiTa with Java, we were able to get an idea of how markov chains produce output based on the inputs given to it. In our example we used poems from W.B. Yeats and sonnets from William Shakespeare. Our hopes were that we would be able to generate poems which  blend elizabethian and modern english to create an illusion of what shakespeare would write if he time travelled to the modern world and learnt modern english.\n",
        "\n",
        "  * We varied the temprature settings to pick up infrequent words with higher probability to see the results but that resulted in unusual outputs with no coherence.\n",
        "\n",
        "  * Based on our results, the output did not generate the text exactly what we were looking for. But the form of speech was good without structure. An example of the text generated by our Markov chains are:\n",
        "\n",
        "\n",
        "    (2-gram)\n",
        "    Mere dreams - and my verse alone did decree That then,\n",
        "    While up, bodies broken rings Upon the light, for thy dear perpetual dulness.\n",
        "    And what good; Wherefore I dropped the moon may deserve to the heights of my soul\n",
        "    Does Minnaloushe runs to work of his beams assemble?\n",
        "    The first of thralled discontent, I, The pacing to that men as sweet-season'd showers\n",
        "    There lives th 'eyes, and poets can understand.\n",
        "\n",
        "    (3-gram)\n",
        "    The mortal moon hath her wish, thou mine, mine eyes best see, Despite of wrinkles, this thy golden time.\n",
        "    What merit do I not spend Revenge upon myself, but with divining eyes, seeing this, let him bring forth Eternal numbers to outlive long date.\n",
        "    Then can I grieve at grievances foregone, And stars began to peep.\n",
        "    This other man I had dreamed A drunken, vainglorious lout.\n",
        "    Excuse not silence so, To swear against the cold, Bare ruined choirs, where thou art?\n",
        "\n",
        "* We have decided to use a neural network, specifically a Recurrent Neural Network (RNN). A recurrent neural network fits well for text generation as its structure defines it to uses information persistence to aloow information to be passed from one step to another. Normal neural networks have this exact shortcoming.\n",
        "\n",
        "  * In RNNs, a special neural network which uses long short term memory, called LSTM, which we have decided to explore. As a RNN, LSTM have the same structure of self looping nodes with an addition of a cell state which helps pass information through its iterations.\n",
        "\n",
        "  * Tensorflow - This is required to import the LSTM neural network model and other tools to help format data\n",
        "  * Pandas - Used to pre-process data which is then supplied to the neural network\n",
        "\n",
        "  * For our data, it was initially just a set of poems from W.B. Yeats totalling 3000 words. This caused some nice outputs but the model broke down at about 30 words after which words would repeat often\n",
        "\n",
        "  * Update: By increasing the epochs from 50 to 100 resulted in better text generation. In terms of creativity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfOq6IkS1NGB",
        "outputId": "b91b2454-8d56-4988-ba74-9bf4e0bf6ae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Words: 2621\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 12, 100)           262200    \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 12, 300)           301200    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 12, 300)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2621)              264721    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2622)              6874884   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7863405 (30.00 MB)\n",
            "Trainable params: 7863405 (30.00 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 57s 169ms/step - loss: 6.9733 - accuracy: 0.0572\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 51s 171ms/step - loss: 6.4524 - accuracy: 0.0583\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 50s 166ms/step - loss: 6.2728 - accuracy: 0.0639\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 57s 190ms/step - loss: 6.1366 - accuracy: 0.0723\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 57s 191ms/step - loss: 6.0317 - accuracy: 0.0774\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 50s 166ms/step - loss: 5.9274 - accuracy: 0.0855\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 51s 168ms/step - loss: 5.8316 - accuracy: 0.0899\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 50s 168ms/step - loss: 5.7396 - accuracy: 0.0939\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 51s 170ms/step - loss: 5.6458 - accuracy: 0.0982\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 52s 174ms/step - loss: 5.5532 - accuracy: 0.1069\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 52s 173ms/step - loss: 5.4512 - accuracy: 0.1063\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 51s 170ms/step - loss: 5.3516 - accuracy: 0.1105\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 51s 169ms/step - loss: 5.2521 - accuracy: 0.1182\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 52s 173ms/step - loss: 5.1663 - accuracy: 0.1229\n",
            "Epoch 15/100\n",
            "300/300 [==============================] - 53s 178ms/step - loss: 5.0821 - accuracy: 0.1269\n",
            "Epoch 16/100\n",
            "300/300 [==============================] - 54s 179ms/step - loss: 4.9884 - accuracy: 0.1304\n",
            "Epoch 17/100\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 4.9017 - accuracy: 0.1390\n",
            "Epoch 18/100\n",
            "300/300 [==============================] - 53s 177ms/step - loss: 4.8115 - accuracy: 0.1454\n",
            "Epoch 19/100\n",
            "300/300 [==============================] - 57s 191ms/step - loss: 4.7252 - accuracy: 0.1496\n",
            "Epoch 20/100\n",
            "300/300 [==============================] - 53s 175ms/step - loss: 4.6277 - accuracy: 0.1596\n",
            "Epoch 21/100\n",
            "300/300 [==============================] - 52s 174ms/step - loss: 4.5360 - accuracy: 0.1659\n",
            "Epoch 22/100\n",
            "300/300 [==============================] - 52s 174ms/step - loss: 4.4519 - accuracy: 0.1780\n",
            "Epoch 23/100\n",
            "300/300 [==============================] - 58s 192ms/step - loss: 4.3607 - accuracy: 0.1874\n",
            "Epoch 24/100\n",
            "300/300 [==============================] - 57s 189ms/step - loss: 4.2636 - accuracy: 0.1976\n",
            "Epoch 25/100\n",
            "300/300 [==============================] - 57s 189ms/step - loss: 4.1767 - accuracy: 0.2038\n",
            "Epoch 26/100\n",
            "300/300 [==============================] - 51s 172ms/step - loss: 4.0939 - accuracy: 0.2169\n",
            "Epoch 27/100\n",
            "300/300 [==============================] - 59s 196ms/step - loss: 3.9952 - accuracy: 0.2264\n",
            "Epoch 28/100\n",
            "300/300 [==============================] - 62s 205ms/step - loss: 3.9083 - accuracy: 0.2399\n",
            "Epoch 29/100\n",
            "300/300 [==============================] - 65s 215ms/step - loss: 3.8122 - accuracy: 0.2578\n",
            "Epoch 30/100\n",
            "300/300 [==============================] - 57s 189ms/step - loss: 3.7225 - accuracy: 0.2717\n",
            "Epoch 31/100\n",
            "300/300 [==============================] - 54s 180ms/step - loss: 3.6316 - accuracy: 0.2906\n",
            "Epoch 32/100\n",
            "300/300 [==============================] - 51s 169ms/step - loss: 3.5496 - accuracy: 0.3056\n",
            "Epoch 33/100\n",
            "300/300 [==============================] - 51s 168ms/step - loss: 3.4685 - accuracy: 0.3194\n",
            "Epoch 34/100\n",
            "300/300 [==============================] - 50s 167ms/step - loss: 3.3928 - accuracy: 0.3405\n",
            "Epoch 35/100\n",
            "300/300 [==============================] - 50s 167ms/step - loss: 3.3059 - accuracy: 0.3545\n",
            "Epoch 36/100\n",
            "300/300 [==============================] - 50s 168ms/step - loss: 3.2254 - accuracy: 0.3687\n",
            "Epoch 37/100\n",
            "300/300 [==============================] - 50s 166ms/step - loss: 3.1588 - accuracy: 0.3900\n",
            "Epoch 38/100\n",
            "300/300 [==============================] - 49s 163ms/step - loss: 3.0823 - accuracy: 0.4029\n",
            "Epoch 39/100\n",
            "300/300 [==============================] - 50s 167ms/step - loss: 3.0165 - accuracy: 0.4239\n",
            "Epoch 40/100\n",
            "300/300 [==============================] - 50s 168ms/step - loss: 2.9471 - accuracy: 0.4342\n",
            "Epoch 41/100\n",
            "300/300 [==============================] - 50s 168ms/step - loss: 2.8767 - accuracy: 0.4505\n",
            "Epoch 42/100\n",
            "300/300 [==============================] - 50s 168ms/step - loss: 2.7995 - accuracy: 0.4732\n",
            "Epoch 43/100\n",
            "300/300 [==============================] - 50s 168ms/step - loss: 2.7456 - accuracy: 0.4839\n",
            "Epoch 44/100\n",
            "300/300 [==============================] - 52s 172ms/step - loss: 2.6827 - accuracy: 0.5026\n",
            "Epoch 45/100\n",
            "300/300 [==============================] - 55s 185ms/step - loss: 2.6368 - accuracy: 0.5043\n",
            "Epoch 46/100\n",
            "300/300 [==============================] - 53s 178ms/step - loss: 2.5822 - accuracy: 0.5214\n",
            "Epoch 47/100\n",
            "300/300 [==============================] - 53s 176ms/step - loss: 2.5218 - accuracy: 0.5371\n",
            "Epoch 48/100\n",
            "300/300 [==============================] - 52s 174ms/step - loss: 2.4709 - accuracy: 0.5471\n",
            "Epoch 49/100\n",
            "300/300 [==============================] - 54s 180ms/step - loss: 2.4149 - accuracy: 0.5612\n",
            "Epoch 50/100\n",
            "300/300 [==============================] - 60s 199ms/step - loss: 2.3737 - accuracy: 0.5725\n",
            "Epoch 51/100\n",
            "300/300 [==============================] - 53s 176ms/step - loss: 2.3149 - accuracy: 0.5842\n",
            "Epoch 52/100\n",
            "300/300 [==============================] - 52s 174ms/step - loss: 2.2846 - accuracy: 0.5848\n",
            "Epoch 53/100\n",
            "300/300 [==============================] - 60s 200ms/step - loss: 2.2386 - accuracy: 0.5977\n",
            "Epoch 54/100\n",
            "300/300 [==============================] - 59s 195ms/step - loss: 2.1844 - accuracy: 0.6100\n",
            "Epoch 55/100\n",
            "300/300 [==============================] - 61s 204ms/step - loss: 2.1594 - accuracy: 0.6160\n",
            "Epoch 56/100\n",
            "300/300 [==============================] - 53s 178ms/step - loss: 2.1191 - accuracy: 0.6258\n",
            "Epoch 57/100\n",
            "300/300 [==============================] - 55s 185ms/step - loss: 2.0775 - accuracy: 0.6327\n",
            "Epoch 58/100\n",
            "300/300 [==============================] - 62s 206ms/step - loss: 2.0429 - accuracy: 0.6379\n",
            "Epoch 59/100\n",
            "300/300 [==============================] - 68s 226ms/step - loss: 2.0167 - accuracy: 0.6409\n",
            "Epoch 60/100\n",
            "300/300 [==============================] - 54s 179ms/step - loss: 1.9783 - accuracy: 0.6522\n",
            "Epoch 61/100\n",
            "300/300 [==============================] - 52s 172ms/step - loss: 1.9319 - accuracy: 0.6597\n",
            "Epoch 62/100\n",
            "300/300 [==============================] - 51s 172ms/step - loss: 1.9079 - accuracy: 0.6690\n",
            "Epoch 63/100\n",
            "300/300 [==============================] - 51s 171ms/step - loss: 1.8959 - accuracy: 0.6645\n",
            "Epoch 64/100\n",
            "300/300 [==============================] - 52s 172ms/step - loss: 1.8484 - accuracy: 0.6751\n",
            "Epoch 65/100\n",
            "300/300 [==============================] - 52s 175ms/step - loss: 1.8107 - accuracy: 0.6834\n",
            "Epoch 66/100\n",
            "300/300 [==============================] - 61s 203ms/step - loss: 1.7821 - accuracy: 0.6876\n",
            "Epoch 67/100\n",
            "300/300 [==============================] - 57s 191ms/step - loss: 1.7571 - accuracy: 0.6898\n",
            "Epoch 68/100\n",
            "300/300 [==============================] - 62s 207ms/step - loss: 1.7355 - accuracy: 0.6979\n",
            "Epoch 69/100\n",
            "300/300 [==============================] - 53s 177ms/step - loss: 1.7109 - accuracy: 0.6976\n",
            "Epoch 70/100\n",
            "300/300 [==============================] - 52s 174ms/step - loss: 1.6963 - accuracy: 0.7026\n",
            "Epoch 71/100\n",
            "300/300 [==============================] - 52s 173ms/step - loss: 1.6658 - accuracy: 0.7079\n",
            "Epoch 72/100\n",
            "300/300 [==============================] - 53s 177ms/step - loss: 1.6528 - accuracy: 0.7101\n",
            "Epoch 73/100\n",
            "300/300 [==============================] - 53s 177ms/step - loss: 1.6208 - accuracy: 0.7202\n",
            "Epoch 74/100\n",
            "300/300 [==============================] - 52s 174ms/step - loss: 1.6131 - accuracy: 0.7172\n",
            "Epoch 75/100\n",
            "300/300 [==============================] - 51s 171ms/step - loss: 1.5770 - accuracy: 0.7259\n",
            "Epoch 76/100\n",
            "300/300 [==============================] - 52s 173ms/step - loss: 1.5650 - accuracy: 0.7258\n",
            "Epoch 77/100\n",
            "300/300 [==============================] - 52s 172ms/step - loss: 1.5354 - accuracy: 0.7306\n",
            "Epoch 78/100\n",
            "300/300 [==============================] - 52s 173ms/step - loss: 1.5141 - accuracy: 0.7382\n",
            "Epoch 79/100\n",
            "300/300 [==============================] - 51s 170ms/step - loss: 1.4949 - accuracy: 0.7426\n",
            "Epoch 80/100\n",
            "300/300 [==============================] - 51s 170ms/step - loss: 1.4664 - accuracy: 0.7459\n",
            "Epoch 81/100\n",
            "300/300 [==============================] - 51s 171ms/step - loss: 1.4544 - accuracy: 0.7493\n",
            "Epoch 82/100\n",
            "300/300 [==============================] - 51s 171ms/step - loss: 1.4556 - accuracy: 0.7454\n",
            "Epoch 83/100\n",
            "300/300 [==============================] - 51s 169ms/step - loss: 1.4337 - accuracy: 0.7504\n",
            "Epoch 84/100\n",
            "300/300 [==============================] - 51s 169ms/step - loss: 1.4053 - accuracy: 0.7573\n",
            "Epoch 85/100\n",
            "300/300 [==============================] - 51s 170ms/step - loss: 1.3933 - accuracy: 0.7555\n",
            "Epoch 86/100\n",
            "300/300 [==============================] - 51s 171ms/step - loss: 1.3804 - accuracy: 0.7575\n",
            "Epoch 87/100\n",
            "300/300 [==============================] - 51s 171ms/step - loss: 1.3610 - accuracy: 0.7626\n",
            "Epoch 88/100\n",
            "300/300 [==============================] - 51s 170ms/step - loss: 1.3526 - accuracy: 0.7647\n",
            "Epoch 89/100\n",
            "300/300 [==============================] - 51s 171ms/step - loss: 1.3318 - accuracy: 0.7675\n",
            "Epoch 90/100\n",
            "300/300 [==============================] - 51s 171ms/step - loss: 1.3211 - accuracy: 0.7695\n",
            "Epoch 91/100\n",
            "300/300 [==============================] - 51s 170ms/step - loss: 1.2951 - accuracy: 0.7734\n",
            "Epoch 92/100\n",
            "300/300 [==============================] - 51s 171ms/step - loss: 1.2890 - accuracy: 0.7766\n",
            "Epoch 93/100\n",
            "300/300 [==============================] - 51s 170ms/step - loss: 1.2967 - accuracy: 0.7729\n",
            "Epoch 94/100\n",
            "300/300 [==============================] - 51s 169ms/step - loss: 1.2772 - accuracy: 0.7768\n",
            "Epoch 95/100\n",
            "300/300 [==============================] - 51s 169ms/step - loss: 1.2550 - accuracy: 0.7832\n",
            "Epoch 96/100\n",
            "300/300 [==============================] - 50s 168ms/step - loss: 1.2430 - accuracy: 0.7836\n",
            "Epoch 97/100\n",
            "300/300 [==============================] - 50s 168ms/step - loss: 1.2186 - accuracy: 0.7858\n",
            "Epoch 98/100\n",
            "300/300 [==============================] - 50s 166ms/step - loss: 1.2078 - accuracy: 0.7905\n",
            "Epoch 99/100\n",
            "300/300 [==============================] - 51s 171ms/step - loss: 1.2080 - accuracy: 0.7901\n",
            "Epoch 100/100\n",
            "300/300 [==============================] - 51s 169ms/step - loss: 1.2135 - accuracy: 0.7868\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras.utils as ku\n",
        "from wordcloud import WordCloud\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "data = open('sample_data/poems.txt',encoding=\"utf-8\").read();\n",
        "\n",
        "# Generating the corpus by\n",
        "# splitting the text into lines\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "# get tokens out from lines\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "# Vocabulary count of the corpus\n",
        "total_words = len(tokenizer.word_index)\n",
        "\n",
        "# Converting the text into embeddings\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences,\n",
        "                                         maxlen=max_sequence_len,\n",
        "                                         padding='pre'))\n",
        "predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "label = ku.to_categorical(label, num_classes=total_words+1)\n",
        "\n",
        "\n",
        "# Building a Bi-Directional LSTM Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words+1, 100,\n",
        "                    input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words+1/2, activation='relu',\n",
        "                kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(total_words+1, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "#train model\n",
        "history = model.fit(predictors, label, epochs=100, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gdp-s5ZrcDB",
        "outputId": "8c1c38ef-1492-4cf1-f38e-da6d92d2e841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a flow, i like to grow enamelling drowned her mirth right and heaven can not the summer innocence of years and share enamelling loss wed enamelling plain you sleep and a fairy queen that rose with one or stricken deaf and dumb and blind at grief flat and wattles do enamelling enamelling loss loss were the\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"This is a flow, i like to grow\"\n",
        "next_words = 50 - len(seed_length)\n",
        "ouptut_text = \"\"\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences(\n",
        "        [token_list], maxlen=max_sequence_len-1,\n",
        "      padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list,\n",
        "                                        verbose=0), axis=-1)\n",
        "    output_word = \"\"\n",
        "\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}